{"name":"Panns","tagline":"Python Approximate Nearest Neighbor Search in very high dimensional space with optimized indexing.","body":"panns -- Nearest Neighbor Search\r\n================================\r\n\r\npanns stands for \"Python Approximate Nearest Neighbor Search\", which is an optimized python library for searching [approximate k-nearest neighbors](http://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor) in very high dimensional spaces. E.g. one typical use in semantic web is finding the most relevant documents in a big text corpus. Currently, panns supports two distance metrics: Euclidean and cosine.\r\n\r\n```python\r\n\r\nfrom panns import *\r\n\r\np1 = PannsIndex('angular')    # index using cosine distance metric\r\np2 = PannsIndex('euclidean')  # index using Euclidean distance metric\r\n...\r\n```\r\n\r\nTechnically, panns is only a small function module in one of our ongoing projects. The reason we release it as a separate package is we realized it is actually very difficult to find an easy-to-use tool which can perform efficient k-NN search with satisfying accuracy in high dimensional space. High dimensionality in this context refers to those datasets having **hundreds of features**, which is already far beyond the capability of standard [k-d tree](http://en.wikipedia.org/wiki/K-d_tree).\r\n\r\npanns is developed by [Liang Wang](http://cs.helsinki.fi/liang.wang) @ Helsinki University. If you have any questions, you can either contact me via email `liang.wang[at]helsinki.fi` or post in [panns-group](https://groups.google.com/forum/#!forum/panns).\r\n\r\n\r\n## Features\r\n\r\n* Pure python implementation.\r\n* Optimized for large and high-dimension dataset (e.g. > 500).\r\n* Generate small index file with high query accuracy.\r\n* Support Euclidean and cosine distance metrics.\r\n* Support parallel building of indices.\r\n* Small memory usage and index can be shared among processes.\r\n* Support raw, csv and [HDF5](http://www.hdfgroup.org/HDF5/) datasets.\r\n\r\n\r\n\r\n## Installation\r\n\r\nAlgebra operations in panns rely on both [Numpy](http://www.numpy.org/) and [Scipy](http://www.scipy.org/), and HDF5 operations rely on [h5py](http://www.h5py.org/). Please make sure you have these three packages properly installed before using panns. The installation can be done by the following shell commands.\r\n\r\n```bash\r\nsudo pip install numpy --upgrade\r\nsudo pip install scipy --upgrade\r\nsudo pip install h5py --upgrade\r\n```\r\n\r\n\r\nThe installation of panns is very straightforward. You can either install it directly from PyPI (probably the easiest way), or download the source code then install manually.\r\n```bash\r\nsudo pip install panns --upgrade\r\n```\r\n\r\n\r\nIf you are interested in the source code or even want to contribute to make it faster and better, you can clone the code from Github.\r\n```bash\r\ngit clone git@github.com:ryanrhymes/panns.git\r\n```\r\n\r\n\r\n\r\n## Quick Start\r\n\r\npanns assumes that the dataset is a row-based the matrix (e.g. m x n), where each row represents a data point from an n-dimension feature space. The code snippet below first constructs a 1000 by 100 data matrix, then builds an index of 50 binary trees and saves it to a file.\r\n\r\n```python\r\n\r\nfrom panns import *\r\n\r\n# create an index of Euclidean distance\r\np = PannsIndex('euclidean')\r\n\r\n# generate a 1000 x 100 dataset\r\nfor i in xrange(1000):\r\n    v = gaussian_vector(100)\r\n    p.add_vector(v)\r\n\r\n# build an index of 50 trees and save to a file\r\np.build(50)\r\np.save('test.idx')\r\n```\r\n\r\nBesides using `add_vector(v)` function, panns supports multiple ways of loading a dataset. For those extremely large datasets, [HDF5](http://www.hdfgroup.org/HDF5/) is recommended though the building performance will be significantly degraded. However, the performance can be improved by enabling parallel building as shown later.\r\n\r\n```python\r\n# datasets can be loaded in the following ways\r\np.load_matrix(A)                     # load a list of row vectors or a numpy matrix\r\np.load_csv(fname, sep=',')           # load a csv file with specified separator\r\np.load_hdf5(fname, dataset='panns')  # load a HDF5 file with specified dataset\r\n```\r\n\r\nThe saved index can be loaded and shared among different processes for future use. Therefore, the query performance can be further improved by parallelism. The following code loads the previously generated index file, then performs a simple query. The query returns 10 approximate nearest neighbors.\r\n\r\n```python\r\n\r\nfrom panns import *\r\n\r\np = PannsIndex('euclidean')\r\np.load('test.idx')\r\n\r\nv = gaussian_vector(100)\r\nn = p.query(v, 10)\r\n```\r\n\r\n\r\nUsually, building index for a high dimensional dataset can be very time-consuming. panns tries to speed up this process from two perspectives: optimizing the code and taking advantage of the physical resources. If multiple cores are available, parallel building can be easily enabled as follows:\r\n\r\n```python\r\n\r\nfrom panns import *\r\n\r\np = PannsIndex('angular')\r\n\r\n....\r\n\r\np.parallelize(True)\r\np.build()\r\n\r\n```\r\n\r\n\r\n\r\n## Theory In a Nutshell\r\n\r\nSimply put, approximate k-NN in panns is achieved by [random projection](http://en.wikipedia.org/wiki/Locality-sensitive_hashing#Random_projection). The index is built by constructing a binary tree. Each node of the tree represents a scalar-projection of certain data points, which are further divided into two groups (left- and right-child) by comparing to their average. The accuracy can be improved from the following perspective:\r\n\r\n* Place the offset wisely (e.g. at the sample average).\r\n* Choose the projection vector wisely (e.g. random or principle components).\r\n* Use more projections (but longer building time and larger index).\r\n* Use more binary trees (also longer building time and larger index).\r\n\r\nThe accuracy of approximate k-NN is usually achieved at the price of large index. panns aims to find the good trade-off of these two conflicting factors. Different from other libraries, panns reuses the projection vectors among different trees instead of generating a new random vector for each node. This can significantly reduces the index size when the dimension is high and trees are many. At the same time, reusing the projection vectors will not degrade the accuracy (see Evaluation section below).\r\n\r\n\r\n\r\n## Evaluation\r\n\r\nEvaluation in this section is simply done by comparing against Annoy. Annoy is a C++ implementation of similar functionality as panns, it is used in Spotify recommender system. In the evaluation, we used a 5000 x 200 dataset, namely 5000 200-dimension feature vectors. For fair comparison, both Annoy and panns use 128 binary trees, and evaluation was done with two distance metrics (Euclidean and cosine). The following table summarizes the results.\r\n\r\n|            | panns (Euclidean) | Annoy (Euclidean) | panns (cosine) | Annoy (cosine) |\r\n|:----------:|:-----------------:|:-----------------:|:--------------:|:--------------:|\r\n|  Accuracy  |       69.2%       |       48.8%       |      70.1%     |      50.4%     |\r\n| Index Size |       5.4 MB      |       20 MB       |     5.4 MB     |      11 MB     |\r\n\r\n\r\nCompared with Annoy, panns can achieve higher accuracy with much smaller index file. The reason was actually already briefly discussed in \"Theory\" section. Generally speaking, the higher accuracy is achieved by placing the offset at sample average; while the smaller index is achieved by reusing the projection vectors.\r\n\r\nOne thing worth pointing out is the evaluation here is far from thorough and comprehensive, other evaluations are highly welcome and we are always ready to link.\r\n\r\n\r\n\r\n## Discussion\r\n\r\nAny suggestions, questions and related discussions are warmly welcome. You can post and find relevant information in [panns-group](https://groups.google.com/forum/#!forum/panns) .\r\n\r\n\r\n\r\n## Future Work\r\n\r\n* Implement mmap on index file to speed up index loading.\r\n* Improve query performance by parallelism.\r\n* Perform more thorough evaluations.\r\n","google":"UA-51583155-1","note":"Don't delete this file! It's used internally to help with page regeneration."}